ByteDance has open-sourced UI-TARS, a set of state-of-the-art (SOTA) models for controlling computers using visual language modeling (VLM). The release includes two SOTA models, with 7B and 72B parameters, along with a PC/MacOS app to interact with the computer. This move is significant as it outperforms other notable models like GPT-4o and Claude across multiple benchmarks.

#### Technical Content
UI-TARS is an end-to-end GUI agent model based on VLM architecture. It perceives screenshots as input and performs human-like interactions, such as keyboard and mouse operations. The key features of UI-TARS include:

* **Architecture**: Based on Visual Language Model (VLM) architecture, which enables the model to understand visual elements on a screen.
* **Input**: Solely uses screenshots as input, allowing it to perceive and interact with graphical user interfaces.
* **Performance**: Achieves SOTA performance on over 10 GUI benchmarks, demonstrating its capability in understanding and interacting with complex graphical environments.
* **Models**: Two models are provided - a 7B parameter model and a 72B parameter model. The larger model (72B) has shown superior performance compared to other leading models like GPT-4o and Claude across various categories.

The release of UI-TARS is accompanied by an infographic that presents a comprehensive comparison of its performance against GPT-4o and Claude. The infographic highlights the improvement of each model relative to UI-TARS-72B, showcasing its consistent outperformance in all evaluated categories.

#### Examples
For instance, the 72B parameter UI-TARS model has been shown to significantly outperform other models in benchmarks such as Visual Web Bench, MM2Web-Average, ScreenSpot-Pro, ScreenQA-Short, and GPT-4o. This demonstrates its potential for a wide range of applications where GUI interaction is crucial.

#### Key Takeaways and Best Practices
1. **Adoption of VLM Architecture**: The success of UI-TARS highlights the potential of VLM architecture in developing advanced GUI agent models.
2. **Open-Sourcing**: ByteDance's decision to open-source UI-TARS promotes collaboration and advancement in the field, allowing researchers and developers to build upon this technology.
3. **Performance Evaluation**: When evaluating the performance of GUI agent models, consider using a diverse set of benchmarks to get a comprehensive view of their capabilities.

#### References
- **Visual Language Model (VLM)**: A type of model architecture that focuses on understanding visual elements and their context within graphical user interfaces.
- **ByteDance**: The company behind the development and open-sourcing of UI-TARS, contributing significantly to advancements in AI and machine learning technologies.
- **GPT-4o and Claude**: Other notable models in the field of natural language processing and artificial intelligence that UI-TARS has been compared against.
- **GitHub**: A platform where the UI-TARS model and its associated tools are made available for access, collaboration, and further development.
## Source

- Original Tweet: [https://twitter.com/i/web/status/1881938753050329467](https://twitter.com/i/web/status/1881938753050329467)
- Date: 2025-02-26 00:52:15


## Media

### Media 1
![media_0](./media_0.mp4)
**Description:** Video file: media_0.mp4

### Media 2
![media_1](./media_1.jpg)
**Description:** This infographic presents a comprehensive comparison of various SOTA (State-of-the-art) models across three categories: UI-TARS-72B, GPT-4o, and Claude. The top section lists 13 models, while the middle left section displays a bar chart illustrating the relative improvement of each model in relation to UI-TARS-72B.

The bottom right corner features a line graph comparing the performance of these three models across five categories: Visual Web Bench, MM2Web-Average, ScreenSpot-Pro, ScreenQA-Short, and GPT-4o. The data is presented as a percentage of improvement over UI-TARS-72B. Notably, UI-TARS-72B consistently outperforms the other two models in all five categories.

The infographic's background is white, with black text used throughout. The use of distinct colors for each model (blue for UI-TARS-72B and orange for Claude) facilitates easy comparison between the three models.

### Media 3
![media_2](./media_2.jpg)
**Description:** The image presents a concise overview of UI-TARS, an end-to-end GUI agent model based on VLM architecture. The key points are:

• **UI-TARS**
	+ Definition: End-to-end GUI agent model
	+ Architecture: Based on VLM (Visual Language Model)
	+ Purpose: Solely perceives screenshots as input and performs human-like interactions

• **Features**
	+ Performs human-like interactions (e.g., keyboard and mouse operations)
	+ Achieves SOTA performance on 10+ GUI benchmarks
	+ GitHub link provided for further information or access to the model

• **Visual Elements**
	+ White text on a black background, with the title "UI-TARS" in large font at the top center of the image

In summary, the image effectively communicates the core concept and features of UI-TARS, providing a clear understanding of its capabilities and architecture.

*Last updated: 2025-02-26 00:52:15*