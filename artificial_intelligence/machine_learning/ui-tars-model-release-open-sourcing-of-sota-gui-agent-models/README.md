ByteDance has open-sourced UI-TARS, a State-of-the-Art (SOTA) end-to-end GUI agent model based on Visual Language Model (VLM) architecture. The release includes two models, 7B and 72B, along with a PC/MacOS app to control computers using Visual Language Models (VLMs). This move is significant as UI-TARS outperforms other SOTA models like GPT-4o and Claude across 10 benchmarks.

#### Technical Content
UI-TARS is defined as an end-to-end GUI agent model that solely perceives screenshots as input and performs human-like interactions. The key features of UI-TARS include:
* **Architecture**: Based on VLM (Visual Language Model), which enables the model to understand visual inputs and generate corresponding actions.
* **Purpose**: Designed to perform human-like interactions, such as keyboard and mouse operations, solely based on screenshot inputs.
* **Performance**: Achieves SOTA performance on 10+ GUI benchmarks, outperforming other notable models like GPT-4o and Claude.
* **Access**: The model is open-sourced, with a GitHub link provided for further information or access to the model.

The infographic comparison highlights UI-TARS-72B's consistent outperformance of GPT-4o and Claude across five categories:
1. **Visual Web Bench**
2. **MM2Web-Average**
3. **ScreenSpot-Pro**
4. **ScreenQA-Short**
5. **GPT-4o**

The performance is presented as a percentage of improvement over UI-TARS-72B, demonstrating its superior capabilities in GUI understanding and interaction.

#### Examples
For instance, the PC/MacOS app accompanying the UI-TARS model release allows users to control their computers using visual inputs, showcasing the practical application of VLMs in real-world scenarios. The open-sourcing of UI-TARS also encourages community engagement, potentially leading to further innovations and improvements in GUI agent models.

#### Key Takeaways and Best Practices
* **Leverage Open-Sourced Models**: Utilize open-sourced models like UI-TARS for research and development purposes, contributing to the advancement of AI technologies.
* **Explore VLM Applications**: Investigate the potential applications of Visual Language Models in various domains, such as accessibility features or automation tools.
* **Stay Updated with SOTA Developments**: Follow the latest developments in State-of-the-Art models like UI-TARS, GPT-4o, and Claude to stay informed about the evolving AI landscape.

#### References
* [ByteDance's UI-TARS Model Release](https://x.com/_akhaliq/status/1881929068746330432/video/1)
* [UI-TARS GitHub Repository](link-to-github-repo)
* [Visual Language Model (VLM) Architecture](reference-to-vlm-architecture)

By understanding and leveraging the capabilities of UI-TARS and other SOTA models, developers can push the boundaries of AI innovation and create more sophisticated, human-like interfaces.
## Source

- Original Tweet: [https://twitter.com/i/web/status/1881938753050329467](https://twitter.com/i/web/status/1881938753050329467)
- Date: 2025-02-25 16:54:25


## Media

### Media 1
![media_0](./media_0.mp4)
**Description:** Video file: media_0.mp4

### Media 2
![media_1](./media_1.jpg)
**Description:** This infographic presents a comprehensive comparison of various SOTA (State-of-the-art) models across three categories: UI-TARS-72B, GPT-4o, and Claude. The top section lists 13 models, while the middle left section displays a bar chart illustrating the relative improvement of each model in relation to UI-TARS-72B.

The bottom right corner features a line graph comparing the performance of these three models across five categories: Visual Web Bench, MM2Web-Average, ScreenSpot-Pro, ScreenQA-Short, and GPT-4o. The data is presented as a percentage of improvement over UI-TARS-72B. Notably, UI-TARS-72B consistently outperforms the other two models in all five categories.

The infographic's background is white, with black text used throughout. The use of distinct colors for each model (blue for UI-TARS-72B and orange for Claude) facilitates easy comparison between the three models.

### Media 3
![media_2](./media_2.jpg)
**Description:** The image presents a concise overview of UI-TARS, an end-to-end GUI agent model based on VLM architecture. The key points are:

• **UI-TARS**
	+ Definition: End-to-end GUI agent model
	+ Architecture: Based on VLM (Visual Language Model)
	+ Purpose: Solely perceives screenshots as input and performs human-like interactions

• **Features**
	+ Performs human-like interactions (e.g., keyboard and mouse operations)
	+ Achieves SOTA performance on 10+ GUI benchmarks
	+ GitHub link provided for further information or access to the model

• **Visual Elements**
	+ White text on a black background, with the title "UI-TARS" in large font at the top center of the image

In summary, the image effectively communicates the core concept and features of UI-TARS, providing a clear understanding of its capabilities and architecture.

*Last updated: 2025-02-25 16:54:25*