ByteDance has open-sourced UI-TARS, a revolutionary end-to-end GUI agent model based on Visual Language Model (VLM) architecture. This release includes two state-of-the-art (SOTA) models, namely 7B and 72B, along with a PC/MacOS application to control computers using visual language modeling (vLMS). The UI-TARS models have outperformed other SOTA models like GPT-4o and Claude across 10 benchmarks.

#### Technical Content
UI-TARS is designed to perceive screenshots as input and perform human-like interactions, such as keyboard and mouse operations. The model's architecture is based on VLM, which enables it to understand visual elements and context within a graphical user interface (GUI). This allows UI-TARS to achieve SOTA performance on various GUI benchmarks.

The open-sourced models include:

* **UI-TARS-7B**: A smaller model with 7 billion parameters, suitable for deployment on devices with limited computational resources.
* **UI-TARS-72B**: A larger model with 72 billion parameters, offering superior performance and accuracy in understanding complex GUI interactions.

To demonstrate the capabilities of UI-TARS, consider a scenario where a user wants to automate a task on their computer. With UI-TARS, the user can simply provide a screenshot of the desired action, and the model will generate the necessary keyboard and mouse inputs to perform the task.

The PC/MacOS application accompanying the UI-TARS models allows users to control their computers using visual language modeling. This enables a range of applications, from automating repetitive tasks to providing assistive technologies for individuals with disabilities.

#### Key Takeaways and Best Practices
* **State-of-the-art performance**: UI-TARS models have outperformed other SOTA models like GPT-4o and Claude across 10 benchmarks.
* **Visual language modeling**: UI-TARS uses VLM architecture to understand visual elements and context within a GUI.
* **End-to-end GUI agent**: UI-TARS can perceive screenshots as input and perform human-like interactions, such as keyboard and mouse operations.
* **Deployment flexibility**: The open-sourced models can be deployed on devices with varying computational resources, from smaller models like UI-TARS-7B to larger models like UI-TARS-72B.

#### References
* [ByteDance's GitHub repository for UI-TARS](https://github.com/byte-dance/ui-tars)
* [Visual Language Model (VLM) architecture](https://en.wikipedia.org/wiki/Visual_language_model)
* [GPT-4o and Claude models](https://en.wikipedia.org/wiki/GPT-4#GPT-4o_and_Claude)

By open-sourcing UI-TARS, ByteDance has made a significant contribution to the field of artificial intelligence and machine learning. The release of these SOTA models and accompanying application is expected to drive innovation in areas like automation, assistive technologies, and human-computer interaction.

---
**Source**: [Original Tweet](https://twitter.com/i/web/status/1881938753050329467)